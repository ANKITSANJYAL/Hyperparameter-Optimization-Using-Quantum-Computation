# âš›ï¸ Quantum Optimizer for Hyperparameter Tuning

This project benchmarks **Quantum vs Classical hyperparameter tuning** for a binary image classification task using the MNIST dataset (digits 0 vs 1). The tuning explores:

- ğŸ” Classical Grid Search
- âš›ï¸ Quantum Approximate Optimization Algorithm (QAOA)

---

## ğŸ¯ Goal

Can quantum optimization algorithms find optimal hyperparameter configurations **more efficiently** than traditional methods?

We optimize:
- **Learning rate**: `0.01` or `0.1`
- **Number of layers**: `1` or `2`
- **Activation function**: `relu` or `tanh`

Total: `2 Ã— 2 Ã— 2 = 8` combinations

---

## ğŸ› ï¸ Stack

| Component        | Tool Used                        |
|------------------|----------------------------------|
| Dataset          | MNIST (filtered: 0 vs 1)         |
| Classical Model  | `scikit-learn` (MLPClassifier)   |
| Quantum Backend  | `AerSampler` (Qiskit Aer)        |
| Quantum Optimizer| `QAOA` (`qiskit-algorithms`)     |
| Cost Modeling    | `SparsePauliOp` (manual QUBO)    |
| Environment      | Python 3.10 + Conda              |

---

## ğŸ—‚ï¸ Project Structure

.
â”œâ”€â”€ main.py                     # Entry point: runs both classical & quantum search
â”œâ”€â”€ exploration.ipynb           # MNIST image + class balance visualization
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py          # Load + preprocess binary MNIST
â”‚   â”œâ”€â”€ model_runner.py         # MLP model training + accuracy return
â”‚   â”œâ”€â”€ classical_runner.py     # Grid search over all 8 combinations
â”‚   â””â”€â”€ quantum_optimizer.py    # QAOA using SparsePauliOp (Qiskit 2.0+ compatible)

## âœ… Current Status
 MNIST (0 vs 1) data loaded and explored

 Classical grid search implemented and benchmarked

 QAOA-based quantum optimizer implemented using SparsePauliOp

 Runtime and accuracy comparison working

ğŸ”® Next Steps
 Log results to results.csv

 Visualize accuracy & runtime (bar chart)

 Add CLI / Streamlit dashboard

 Deploy on IBM QPU using qiskit-ibm-runtime
